# Test Suite Documentation - Datadis Python SDK

Esta documentaciÃ³n describe la suite de tests comprensiva para el SDK de Python de Datadis, diseÃ±ada para garantizar la calidad, confiabilidad y mantenibilidad del cÃ³digo.

## Estructura de Tests

```
tests/
â”œâ”€â”€ conftest.py                    # ConfiguraciÃ³n global y fixtures
â”œâ”€â”€ test_auth.py                   # Tests de autenticaciÃ³n y sesiones
â”œâ”€â”€ test_client_v1.py             # Tests del cliente V1
â”œâ”€â”€ test_client_v2.py             # Tests del cliente V2
â”œâ”€â”€ test_exceptions.py             # Tests de manejo de errores
â”œâ”€â”€ test_integration.py            # Tests de integraciÃ³n end-to-end
â”œâ”€â”€ test_models.py                 # Tests de modelos Pydantic
â”œâ”€â”€ test_utils.py                  # Tests de funciones utilitarias
â”œâ”€â”€ test_coverage_and_quality.py  # Tests de calidad y cobertura
â”œâ”€â”€ pytest.ini                    # ConfiguraciÃ³n de pytest
â”œâ”€â”€ run_tests.py                  # Script de ejecuciÃ³n de tests
â””â”€â”€ README.md                     # Esta documentaciÃ³n
```

## CategorÃ­as de Tests

### ðŸƒ Tests Unitarios (`unit`)
Tests rÃ¡pidos y aislados que validan funcionalidades especÃ­ficas:
- **Modelos Pydantic**: ValidaciÃ³n, serializaciÃ³n, deserializaciÃ³n
- **AutenticaciÃ³n**: Login, tokens, renovaciÃ³n
- **Utilidades**: Validadores, normalizaciÃ³n de texto, HTTP client
- **Excepciones**: JerarquÃ­a de errores, propagaciÃ³n

### ðŸ”— Tests de IntegraciÃ³n (`integration`)
Tests que validan la interacciÃ³n entre componentes:
- **Workflows completos**: Auth â†’ Supplies â†’ Consumption â†’ Contracts
- **Compatibilidad V1/V2**: Mismos datos, diferentes formatos
- **RecuperaciÃ³n de errores**: Reintentos, renovaciÃ³n de tokens
- **Escenarios reales**: MÃºltiples suministros, gestiÃ³n de propiedades

### ðŸŒ Tests Lentos (`slow`)
Tests que pueden tomar mÃ¡s tiempo:
- **Performance**: Benchmarks, mÃºltiples requests
- **Datasets grandes**: Manejo de muchos registros
- **Timeouts**: Escenarios de tiempo agotado

### [COMPONENTES] Tests por Componente
Tests organizados por Ã¡rea funcional:
- `auth`: AutenticaciÃ³n y manejo de sesiones
- `models`: ValidaciÃ³n de modelos Pydantic
- `client_v1`: Cliente V1 especÃ­fico
- `client_v2`: Cliente V2 especÃ­fico
- `utils`: Funciones utilitarias
- `errors`: Manejo de errores

## EjecuciÃ³n de Tests

### MÃ©todo RÃ¡pido
```bash
# Tests unitarios rÃ¡pidos
python run_tests.py --fast

# Tests de un componente especÃ­fico
python run_tests.py --component auth
python run_tests.py --component models
```

### MÃ©todo Completo
```bash
# Todos los tests
python run_tests.py --full

# Tests con coverage
python run_tests.py --coverage

# Suite completa (tests + quality checks)
python run_tests.py --all
```

### MÃ©todo Manual con pytest
```bash
# Tests bÃ¡sicos
poetry run pytest

# Tests con coverage
poetry run pytest --cov=datadis_python --cov-report=html

# Tests especÃ­ficos
poetry run pytest -m unit                    # Solo unitarios
poetry run pytest -m integration             # Solo integraciÃ³n
poetry run pytest -m "auth and not slow"     # Auth rÃ¡pidos
poetry run pytest tests/test_models.py       # Solo modelos
```

## ConfiguraciÃ³n y Fixtures

### Fixtures Principales (`conftest.py`)

#### Credenciales y AutenticaciÃ³n
- `test_credentials`: Credenciales de prueba
- `test_token`: Token JWT de prueba
- `mock_auth_success`: Mock de autenticaciÃ³n exitosa
- `mock_auth_failure`: Mock de autenticaciÃ³n fallida

#### Clientes
- `v1_client`: Cliente V1 sin autenticar
- `v2_client`: Cliente V2 sin autenticar
- `authenticated_v1_client`: Cliente V1 ya autenticado
- `authenticated_v2_client`: Cliente V2 ya autenticado

#### Datos de Prueba
- `sample_supply_data`: Datos de suministro de ejemplo
- `sample_consumption_data`: Datos de consumo de ejemplo
- `sample_contract_data`: Datos de contrato de ejemplo
- `sample_max_power_data`: Datos de potencia mÃ¡xima
- `sample_distributor_data`: Datos de distribuidor

#### Helpers de ValidaciÃ³n
- `assert_valid_cups`: Validador de cÃ³digos CUPS
- `assert_valid_date_format`: Validador de formato de fecha
- `assert_valid_time_format`: Validador de formato de hora

### Estrategia de Mocking

Todos los tests usan mocking completo de HTTP requests:

```python
# Ejemplo de mock en test
with responses.RequestsMock() as rsps:
    rsps.add(
        responses.GET,
        f"{DATADIS_API_BASE}/get-supplies",
        json=sample_data,
        status=200
    )

    result = client.get_supplies()
    assert len(result) > 0
```

## Coverage y Calidad

### MÃ©tricas de Coverage
- **Target**: >95% de cobertura de cÃ³digo
- **Incluye**: Todo el cÃ³digo en `datladis_python/`
- **Excluye**: Tests, ejemplos, documentaciÃ³n

### Reportes de Coverage
```bash
# Generar reporte HTML
poetry run pytest --cov=datadis_python --cov-report=html
# Ver en: htmlcov/index.html

# Reporte en terminal
poetry run pytest --cov=datadis_python --cov-report=term-missing

# Reporte JSON
poetry run pytest --cov=datadis_python --cov-report=json
```

### Quality Checks
El script `run_tests.py --quality` ejecuta:
- **Black**: Formato de cÃ³digo
- **isort**: Orden de imports
- **flake8**: Linting
- **mypy**: Type checking

## Casos de Test Importantes

### 1. ValidaciÃ³n de Modelos Pydantic

```python
def test_consumption_data_validation(sample_consumption_data):
    # Test creaciÃ³n exitosa
    consumption = ConsumptionData(**sample_consumption_data)
    assert consumption.cups == sample_consumption_data["cups"]

    # Test validaciÃ³n de aliases
    assert consumption.consumption_kwh == sample_consumption_data["consumptionKWh"]

    # Test serializaciÃ³n JSON
    json_str = consumption.model_dump_json(by_alias=True)
    restored = ConsumptionData.model_validate_json(json_str)
    assert restored.consumption_kwh == consumption.consumption_kwh
```

### 2. AutenticaciÃ³n y RenovaciÃ³n de Tokens

```python
def test_token_renewal_on_401(authenticated_v1_client, test_token):
    with responses.RequestsMock() as rsps:
        # Request falla con 401
        rsps.add(responses.GET, url, status=401)

        # Nueva autenticaciÃ³n automÃ¡tica
        rsps.add(responses.POST, auth_url, body=f"{test_token}_renewed", status=200)

        # Request exitoso con nuevo token
        rsps.add(responses.GET, url, json=data, status=200)

        result = client.get_supplies()

        # Verificar renovaciÃ³n automÃ¡tica
        assert client.token == f"{test_token}_renewed"
```

### 3. Flujos de IntegraciÃ³n Completos

```python
def test_complete_workflow_v1(authenticated_v1_client):
    # 1. Obtener distribuidores
    distributors = client.get_distributors()

    # 2. Obtener suministros
    supplies = client.get_supplies()

    # 3. Obtener consumo para cada suministro
    for supply in supplies:
        consumption = client.get_consumption(
            cups=supply.cups,
            distributor_code=supply.distributor_code,
            date_from="2024/01/01",
            date_to="2024/01/31"
        )
        assert len(consumption) > 0
```

### 4. Manejo de Errores

```python
def test_api_error_propagation(authenticated_v1_client):
    with responses.RequestsMock() as rsps:
        rsps.add(responses.GET, url, status=500, json={"error": "Server error"})

        with pytest.raises(APIError) as exc_info:
            client.get_supplies()

        assert exc_info.value.status_code == 500
```

## Mejores PrÃ¡cticas

### [BUENAS PRACTICAS] Hacer
- **Usar fixtures**: Reutilizar configuraciÃ³n comÃºn
- **Tests aislados**: Cada test debe ser independiente
- **Mocking completo**: No hacer requests HTTP reales
- **Assertions especÃ­ficas**: Verificar comportamiento exacto
- **Casos edge**: Probar valores lÃ­mite y errores
- **Documentar tests complejos**: Explicar casos no obvios

### [EVITAR] Evitar
- **Tests dependientes**: No asumir orden de ejecuciÃ³n
- **Requests reales**: Siempre usar mocks
- **Fixtures mutables**: Evitar efectos secundarios
- **Tests lentos**: Mantener tests unitarios rÃ¡pidos
- **CÃ³digo duplicado**: Usar helpers y fixtures
- **Assertions vagas**: Ser especÃ­fico en verificaciones

## Troubleshooting

### Tests Fallan
```bash
# Ver detalles de fallos
pytest -v --tb=long

# Ejecutar test especÃ­fico
pytest tests/test_auth.py::TestV1Authentication::test_successful_authentication -v

# Debug con print
pytest -s tests/test_auth.py
```

### Coverage Bajo
```bash
# Ver lÃ­neas no cubiertas
pytest --cov=datadis_python --cov-report=term-missing

# Generar reporte HTML detallado
pytest --cov=datadis_python --cov-report=html
```

### Tests Lentos
```bash
# Ver duraciÃ³n de tests
pytest --durations=10

# Solo tests rÃ¡pidos
pytest -m "unit and not slow"
```

### Problemas de Imports
```bash
# Verificar instalaciÃ³n
poetry install --with dev

# Verificar Python path
python -c "import datadis_python; print(datadis_python.__file__)"
```

## Contribuir

### AÃ±adir Nuevos Tests

1. **Identificar necesidad**: Â¿QuÃ© funcionalidad falta coverage?
2. **Elegir ubicaciÃ³n**: Â¿Unit, integration, o componente especÃ­fico?
3. **Usar fixtures**: Reutilizar configuraciÃ³n existente
4. **AÃ±adir markers**: Categorizar apropiadamente
5. **Documentar**: Explicar casos complejos

### Ejemplo de Test Nuevo

```python
@pytest.mark.unit
@pytest.mark.client_v1
def test_new_functionality(authenticated_v1_client, sample_data):
    """Test nueva funcionalidad del cliente V1."""
    with responses.RequestsMock() as rsps:
        rsps.add(
            responses.GET,
            "https://datadis.es/api-private/api/new-endpoint",
            json=sample_data,
            status=200
        )

        result = authenticated_v1_client.new_method()

        assert result is not None
        assert len(result) > 0
```

### Mantener Tests

- **Ejecutar regularmente**: `python run_tests.py --all`
- **Actualizar fixtures**: Cuando cambie la API
- **Revisar coverage**: Mantener >95%
- **Optimizar velocidad**: Tests unitarios <0.1s cada uno
- **Documentar cambios**: Actualizar README cuando sea necesario

## IntegraciÃ³n CI/CD

Para integrar en GitHub Actions u otros sistemas CI/CD:

```yaml
# Ejemplo .github/workflows/tests.yml
name: Tests
on: [push, pull_request]
jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - uses: actions/setup-python@v4
        with:
          python-version: '3.9'
      - name: Install dependencies
        run: pip install poetry && poetry install --with dev
      - name: Run quality checks
        run: python run_tests.py --quality
      - name: Run tests with coverage
        run: python run_tests.py --coverage
      - name: Upload coverage
        uses: codecov/codecov-action@v3
```

Este test suite garantiza la calidad y confiabilidad del SDK de Datadis Python, proporcionando una base sÃ³lida para el desarrollo y mantenimiento continuo del proyecto.
